<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Michel Pohl - Blog</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/academicons.min.css"/>
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="stylesheet" href="../../assets/css/github-gist-code-box.css" >
		<link rel="stylesheet" href="../../assets/css/custom_blog_article.css" >
		<script>
			MathJax = {
			  tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
			};
		</script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>


	</head>
	<body class="is-preload">

		<div id="wrapper">

			<!-- Main -->
			<div id="main">
				<div class="inner">

					<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>Michel Pohl </strong> - Personal blog </a>								
					</header>

					<!-- Content -->
					<section>
						<header class="main">
							<h1>Personal selection of AI research articles</h1>
							<h2><i>A collection of well-written articles that I enjoyed reading and helped me grow as an AI professional</i><h2>
						</header>

						<span class="image main"><img src="images/janko-ferlic-library.jpg" style="max-width: 60%;" alt="" <figcaption>Photo by Janko Ferlic on Unsplash </figcaption> </span>

						<p> Reading research articles is an important habit as a professional in the AI field. I confess that I have not read all the important articles in that domain, and in addition, this page is often updated as I try to add information about articles that I previously read. Currently, instead of providing an extensive list of all the seminal AI papers (that I will strive to read and add later), I mention the papers that I have already read and that I feel were a significant stepping stone in my AI journey. I try to strike a balance between all fields of AI, although there might be more papers in the computer vision section as that speciality is an important part of my academic and professional path until now. Even though machine learning is my main interest, I also propose papers outside of that area and that help understand current developments in machine learning or better grasp the foundations of certain subfields of AI (e.g., classical image processing). Last, on top of mentioning very important papers, I will also try to add some papers that are less cited and less well known but that I think are well written and were also part of my personal learning journey in AI. </p>

					<!-- </section> -->

						<h2 id="Deep generative modeling"> Deep generative modeling</h2>

						<p>
							<ul>
								<li>
									Kingma, D. P., & Welling, <a href="https://arxiv.org/abs/1312.6114" target ="_blank">Auto-Encoding Variational Bayes (2013)</a>, Proceedings of the International Conference on Learning Representations (ICLR)
								</il>
							</ul>
						</p>

						<p>Generative modeling is an unsupervised form of machine learning where the model learns to discover the patterns in input data. Among these deep generative models, two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). VAEs are autoencoders that tackle the problem of the latent space irregularity of classical autoencoder neural networks by making the encoder return a distribution over the latent space instead of a single point. The encodings' distribution is regularised during training to ensure that its latent space has good properties allowing us to generate some new
						data. The loss function of VAEs, composed of a reconstruction term and a regularisation term (the Kullback-Leibler divergence between the prior latent distribution and its approximated distribution given the input data) are derived using variational inference. The following articles by Joseph Rocca on Towards Data Science were helpful for understanding the paper: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" target ="_blank">"Understanding Variational Autoencoders (VAEs)"</a> and <a href="https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29" target ="_blank">"Bayesian inference problem, MCMC and variational inference"</a>. I also found the following Stack Exchange post about the calculation of the <a href="https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians" target ="_blank">Kullback-Leibler divergence between two multivariate Gaussians</a> useful.</p>


						<h2 id="Computer Vision"> Computer Vision</h2>

						<h3 id="Classical computer vision"> Classical computer vision </h3>

						<p>
							<ul>
								<li>
									David G. Lowe, <a href="https://nsk.compsciclub.ru/attachments/classes/file_xGZv8pKe/lowe04.pdf" target ="_blank">Distinctive image features from scale-invariant keypoints (2004)</a>, International Journal of Computer Vision
								</il>
							</ul>
						</p>

						<p>This paper presents the scale-invariant feature transform (SIFT), a method to extract feature points and corresponding descriptors (or feature vectors) from images that are invariant to scale and rotation, and robust to affine distorsion, 3D viewpoint change, noise, and illumination. First, a scale-space pyramidal representation of the image is constructed and extremas corresponding to blobs are located in that space using 3D quadratic function optimization after computation of the difference of Gaussians. Keypoints with low contrast and corresponding to strong edges are eliminated. Each keypoint (extrema) is characterized by its 2D position, scale, and orientation, derived from the local orientation histogram. The local image descriptor is composed of the binned local orientation histograms along the keypoint direction around the keypoint for each neighbor box. 

						I found that the videos of Pratik Jain about <a href="https://www.youtube.com/watch?v=kH3IzumEc1Q" target ="_blank">homography</a>, <a href="https://www.youtube.com/watch?v=pwFeeVXLhsE" target ="_blank">image registration</a>, <a href="https://www.youtube.com/watch?v=8aNOzgIbaeA" target ="_blank">the Harris corner detector</a> and <a href="https://www.youtube.com/watch?v=gh7Zrc121Q8" target ="_blank">its properties</a>, and the SIFT <a href="https://www.youtube.com/watch?v=LXk4A24V8mQ" target ="_blank">invariant features</a> and <a href="https://www.youtube.com/watch?v=LWSx0p_PAS8" target ="_blank">feature descriptors</a> were very helpful to understand the context of the paper and its concepts.
						</p>

						<h3 id="Computer vision with deep learning"> Computer vision with deep learning </h3>

						<p>
							<ul>
								<li>
									Jonathan Long et al., <a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" target ="_blank">Fully convolutional networks for semantic segmentation (2015)</a>, Proceedings of the IEEE conference on computer vision and pattern recognition
								</il>
							</ul>
						</p>

						<p> This paper uses fully convolutional networks (FCN), that is, networks that only use convolutions and no fully connected layers, to perform segmentation of natural images. In opposition to classical ConvNets for image classification, FCNs do not have a fixed input size image; they also have low time complexity. 1D convolutions are used to replace fully connected layers to produce a coarse heat map where each channel represents a class, and “deconvolution” or upsampling layers are used to map features at the coarse level to larger 2D output segmentation results. Combining the information from features at different depths in the network using skip connections help refine the dense output by adding localization information to more content-related features. I found <a href="https://d2l.ai/chapter_computer-vision/fcn.html" target ="_blank">the implementation in the d2l.ai book</a> helpful to understand the paper, although the skip layers were not implemented.
						</p>

						<p>
							<ul>
								<li>
									Olaf Ronneberger et al., <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf" target ="_blank">U-Net: Convolutional Networks for Biomedical Image Segmentation (2015)</a>, International Conference on Medical image computing and computer-assisted intervention
								</il>
							</ul>
						</p>

						<p> U-Net is a neural network for image segmentation that uses relatively symmetric contracting and expanding paths (that hence form a U shape) with skip connections. That architecture leads to a good balance between localization accuracy and the use of context, while keeping the computing cost low. It is based on the FCN architecture, but the difference is that the U-Net has many feature channels in the up-sampling part. 
						</p>


						<p>
							<ul>
								<li>
									Tsung-Yi Lin et al., <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf" target ="_blank">Feature Pyramid Networks for Object Detection (2017)</a>, Proceedings of the IEEE conference on computer vision and pattern recognition
								</il>
							</ul>
						</p>

						<p> This paper introduces feature pyramid networks (FPN), a framework that uses the inherent multi-scale pyramidal hierarchy of ConvNets with low-resolution semantically strong features and high-resolution but semantically weak features, to construct a feature pyramid that has strong semantics at all scales. The bottom-up pathway of the FPN is the feed-forward computation of the backbone ConvNet, computing feature maps at several scales. The subsequent top-down pathway upsamples the feature map from the highest level, that lateral connections enhance at each level by element-wise addition. Prediction is performed at each scale of the bottom-down path. The authors adapt Region Proposal Network (RPN) and Fast R-CNN to the FCN framework respectively for bounding box proposal generation and object detection, and also show high performance with segmentation.
						</p>

						<!-- <p>
							<ul>
								<li>
									He et al., <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf" target ="_blank">Momentum contrast for unsupervised visual representation learning (2020)</a>, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
								</il>
							</ul>
						</p>

						<p> Unsupervised representation learning is successful in natural language processing, but supervised pre-training still prevails in computer vision. This paper introduces Momentum Contrast (MoCo) for unsupervised visual representation learning. A dynamic dictionary with a queue and moving-averaged encoder are built to make contrastive unsupervised learning easier. This paper follows an instance discrimination task: a query matches a dictionary key if they are encoded views of the same image.  <i>Details to be added later</i> 
						</p> -->

						<p>
							<ul>
								<li>
									Chen et al., <a href="https://arxiv.org/abs/2002.05709" target ="_blank">A Simple Framework for Contrastive Learning of Visual Representations (2020)</a>, International conference on machine learning
								</il>
							</ul>
						</p>

						<p> Unsupervised representation learning is successful in natural language processing, but supervised pre-training still prevails in computer vision. Furthermore, constructing large scale labeled datasets is a difficult task, so self-supervised learning could be useful in computer vision. In the SimCLR method, data is augmented using random cropping, resizing, and color distorsion to form positive and negative pairs of images. Two functions <i>f</i> and <i>g</i> are learned, where <i>f</i> corresponds to the learned representation and <i>g</i> is the “projection head” such that the contrastive loss function is minimized for <i>(g o f)</i>. That loss function is the normalized cross entropy loss with adjustable temperature. One of the key findings is that unsupervised learning seems to benefit more from scaling up (model size, batch size, training epochs, data augmentation) than supervised learning.
						</p>


						<i>Published on January 5, 2023, last update on February 5, 2023</i>

					</section>

				</div>
			</div>

			<!-- Sidebar -->
			<div id="sidebar">
				<div class="inner">


					
					<!-- Menu -->
					<nav id="menu">
						<header class="major">
							<h2>Menu</h2>
						</header>

						<ul>
							<li><a href="../../index.html" target="_blank"><b>Blog homepage</b></a></li>
						</ul>

						<hr />
						<br />

						<ul>
							<li><a href="#header">Page top</a></li>
							<li><a href="#Deep generative modeling">Deep generative modeling</a></li>
							<li>
								<span class="opener"> Computer Vision</span>
								<ul>
									<li><a href="#Classical computer vision"> Classical computer vision</a></li>
									<li><a href="#Computer vision with deep learning"> Computer vision with deep learning</a></li>
								</ul>
							</li>
						</ul>
					</nav>

					<!-- Section -->
					<section>
						<header class="major">
							<h2>Suggested reading</h2>
						</header>
						<div class="mini-posts">
							<article>
								<i class="ai ai-arxiv ai-7x"></i><br>
								The version of my published article openly accessible on ArXiv <a href=" 	
								https://doi.org/10.48550/arXiv.2106.01100" target ="_blank">(link here)</a>
							</article>
						</div>
					</section>

					<!-- Section -->
					<section>
						<header class="major">
							<h2>Contact</h2>
						</header>
						<ul class="contact">
							<li class="icon solid fa-envelope"><a href="#">michel.pohl@centrale-marseille.fr</a></li>
							<li class="icon solid fa-home">Oxford, United Kingdom</li>
						</ul>
					</section>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; 2022 Michel Pohl. All rights reserved. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

				</div>
			</div>


		</div>

		<!-- Scripts -->
		<script src="../../assets/js/jquery.min.js"></script>
		<script src="../../assets/js/browser.min.js"></script>
		<script src="../../assets/js/breakpoints.min.js"></script>
		<script src="../../assets/js/util.js"></script>
		<script src="../../assets/js/main.js"></script>

	</body>
</html>